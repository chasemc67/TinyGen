{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import asyncio\n",
    "from dataclasses import dataclass\n",
    "from langchains.openai import run_llm_chain_for_input\n",
    "from integrations.github import GithubFile\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ProcessedGithubFile:\n",
    "    originalFile: GithubFile\n",
    "    llmResponse: str\n",
    "\n",
    "mock_github_results: List[GithubFile] = [\n",
    "    GithubFile(\"test1.txt\", \"This is a test file\"),\n",
    "    GithubFile(\"test2.txt\", \"This is another test file\"),\n",
    "    GithubFile(\"test3.txt\", \"This file has a dog in it\"),\n",
    "    GithubFile(\"test4.txt\", \"This file has a cat in it\"),\n",
    "    GithubFile(\"test5.txt\", \"This file has a golden retriever in it\"),\n",
    "    GithubFile(\"test6.txt\", \"This is yet another test file\"),\n",
    "]\n",
    "mock_user_prompt = \"find dogs within the files\"\n",
    "\n",
    "\n",
    "# Define the function you want to run on each object:\n",
    "async def process_item(item: GithubFile) -> ProcessedGithubFile:\n",
    "    # This is a placeholder function that doesn't do much.\n",
    "    # Replace it with whatever function you want to run.\n",
    "    # await asyncio.sleep(1)  # simulate IO delay\n",
    "    # return ProcessedGithubFile(item, f\"Processed {item.filename} with code {item.content}\")\n",
    "    #return f\"Processed {item.filename} with code {item.content}\"\n",
    "\n",
    "    result = run_llm_chain_for_input(mock_user_prompt, item.content)\n",
    "    return ProcessedGithubFile(item, result)\n",
    "\n",
    "\n",
    "\n",
    "# TODO since I think this syntax is dependent on the python version\n",
    "# We should find a way to define it such that heroku and local development will respect/enforce it\n",
    "async def process_all_items(items: List[GithubFile]) -> List[ProcessedGithubFile]:\n",
    "    # Create a list of tasks to run:\n",
    "    tasks = [process_item(item) for item in items]\n",
    "\n",
    "    # Run the tasks:\n",
    "    #loop = asyncio.get_event_loop()\n",
    "    # results = loop.run_until_complete(asyncio.gather(*tasks))\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Do something with the results:\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ProcessedGithubFile(originalFile=GithubFile(filename='test1.txt', content='This is a test file'), llmResponse='\\n\\n0'), ProcessedGithubFile(originalFile=GithubFile(filename='test2.txt', content='This is another test file'), llmResponse='\\n\\n50'), ProcessedGithubFile(originalFile=GithubFile(filename='test3.txt', content='This file has a dog in it'), llmResponse='\\n\\n100'), ProcessedGithubFile(originalFile=GithubFile(filename='test4.txt', content='This file has a cat in it'), llmResponse='\\n\\n0'), ProcessedGithubFile(originalFile=GithubFile(filename='test5.txt', content='This file has a golden retriever in it'), llmResponse='\\n\\n100'), ProcessedGithubFile(originalFile=GithubFile(filename='test6.txt', content='This is yet another test file'), llmResponse='\\n\\n50'), ProcessedGithubFile(originalFile=GithubFile(filename='test1.txt', content='This is a test file'), llmResponse='\\n\\n0'), ProcessedGithubFile(originalFile=GithubFile(filename='test2.txt', content='This is another test file'), llmResponse='\\n\\n50'), ProcessedGithubFile(originalFile=GithubFile(filename='test3.txt', content='This file has a dog in it'), llmResponse='\\n\\n100'), ProcessedGithubFile(originalFile=GithubFile(filename='test4.txt', content='This file has a cat in it'), llmResponse='\\n\\n0'), ProcessedGithubFile(originalFile=GithubFile(filename='test5.txt', content='This file has a golden retriever in it'), llmResponse='\\n\\n100'), ProcessedGithubFile(originalFile=GithubFile(filename='test6.txt', content='This is yet another test file'), llmResponse='\\n\\n50'), ProcessedGithubFile(originalFile=GithubFile(filename='test1.txt', content='This is a test file'), llmResponse='\\n\\n0'), ProcessedGithubFile(originalFile=GithubFile(filename='test2.txt', content='This is another test file'), llmResponse='\\n\\n50'), ProcessedGithubFile(originalFile=GithubFile(filename='test3.txt', content='This file has a dog in it'), llmResponse='\\n\\n100'), ProcessedGithubFile(originalFile=GithubFile(filename='test4.txt', content='This file has a cat in it'), llmResponse='\\n\\n0'), ProcessedGithubFile(originalFile=GithubFile(filename='test5.txt', content='This file has a golden retriever in it'), llmResponse='\\n\\n100'), ProcessedGithubFile(originalFile=GithubFile(filename='test6.txt', content='This is yet another test file'), llmResponse='\\n\\n50')]\n"
     ]
    }
   ],
   "source": [
    "results = await process_all_items(mock_github_results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from integrations.github import download_github_data_to_file, get_github_files_and_contents\n",
    "\n",
    "files = get_github_files_and_contents(\"https://github.com/chasemc67/TinyGen\")\n",
    "download_github_data_to_file(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GithubFile(filename='.gitignore', content='# Config secrets\\n.env\\n\\n# Python cache files\\n__pycache__/\\n*.py[cod]\\n\\n# Virtual environment\\nvenv/\\n\\n# Project-specific settings\\n.env  # environment variables, never commit this\\n*.log  # log files\\n\\n# Byte-compiled / optimized / DLL files\\n*.py[cod]\\n*$py.class\\n\\n# Distribution / packaging\\n.Python\\nbuild/\\ndevelop-eggs/\\ndist/\\ndownloads/\\neggs/\\n.eggs/\\nlib/\\nlib64/\\nparts/\\nsdist/\\nvar/\\n*.egg-info/\\n.installed.cfg\\n*.egg\\n\\n# PyCharm files\\n.idea/\\n*.iml'), GithubFile(filename='Procfile', content='web: uvicorn main:app --host=0.0.0.0 --port=${PORT:-5000}'), GithubFile(filename='README.md', content='# TinyGen\\n\\nA simple API to generate code diffs using ChatGPT\\n\\n## Running the Application Locally\\n\\n1. First, clone the repository to your local machine and navigate to the project directory:\\n\\n```bash\\ngit clone <repository-url>\\ncd <project-directory>\\nsource venv/bin/activate\\npip install -r requirements.txt\\n```\\n\\nTeardown:\\n```bash\\ndeactivate\\n```\\n\\n2. create a new .env file for secrets, you can get this information from [supabase](https://supabase.com/dashboard/project/fvvmbtjoztejtalynctc/settings/api)\\nyour .env file should look like this:\\n```\\nSUPABASE_URL=your_supabase_url\\nSUPABASE_API_KEY=your_supabase_api_key\\n```\\n\\n\\n3. Run the app\\n```bash\\nuvicorn main:app --reload\\n```\\n\\nNow, the server should be up and running at http://localhost:8000. You can view the interactive API documentation at http://localhost:8000/docs.\\n\\nTo test your server, you can run the following CURL command:\\n```bash\\ncurl http://localhost:8000/test\\n```\\n\\nTo make a proper request to generate a response, and record the input and output in supabase, run the following CURL\\n```bash\\ncurl -X POST -H \"Content-Type: application/json\" -d \\'{\"url\": \"yourgitrepo.url\", \"prompt\": \"your prompt\"}\\' http://127.0.0.1:8000/generate\\n```\\n\\n\\n## Deploy changes to prod\\nUse Heroku for production:\\n```bash\\nheroku login\\ngit push heroku main\\nheroku open\\nheroku logs --tail\\n```\\n\\n\\n### Python things:\\n```bash\\nsource venv/bin/activate # activate virtual env\\ndeactive # deactivate virtual env\\npip freeze > requirements.txt # update deps\\n```'), GithubFile(filename='config.py', content='from dotenv import load_dotenv\\nimport os\\n\\nload_dotenv() # load local .env file if it exists for local development\\n\\nsupabase_url = os.getenv(\"SUPABASE_URL\")\\nsupabase_api_key = os.getenv(\"SUPABASE_API_KEY\")\\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\")'), GithubFile(filename='main.py', content='from fastapi import FastAPI\\nfrom app.models import RequestRecord\\nfrom db.supabase import supabase_record_request\\nfrom langchains.openai import create_openai_llm\\nfrom integrations.github import get_github_files_and_contents\\n\\napp = FastAPI()\\n\\nllm = create_openai_llm()\\n\\n@app.get(\"/test\")\\ndef read_test():\\n    return {\"message\": \"hello\"}\\n\\n@app.get(\"/testllm\")\\ndef test_llm():\\n    return llm.predict(\"What is a cool name for a dog?\")\\n\\n@app.get(\"/testgithub\")\\ndef test_github():\\n    return get_github_files_and_contents(\\'https://github.com/chasemc67/TinyGen\\')\\n\\n# Append to the history table log\\n@app.post(\"/generate\")\\ndef generate(request: RequestRecord):\\n    try:\\n        # Add request into history table\\n        request = supabase_record_request(request)\\n\\n        if request:\\n            print(\"Request recorded successfully\")\\n        else: \\n            return {\"message\": \"Request recording failed\"}\\n    except Exception as e:\\n        print(\"Error: \", e)\\n        return {\"message\": \"Request recording failed\"}\\n\\n    # TODO get files from github to pass to LLM (or invoke LLM now with custom github tool)\\n\\n    return {\"message\": \"Request recording succeeded\"}'), GithubFile(filename='requirements.txt', content='aiohttp==3.8.4\\naiosignal==1.3.1\\nannotated-types==0.5.0\\nanyio==3.7.1\\nasync-timeout==4.0.2\\nattrs==23.1.0\\nbcrypt==4.0.1\\nbleach==6.0.0\\nblis==0.7.9\\ncatalogue==2.0.8\\ncertifi==2023.5.7\\ncharset-normalizer==3.2.0\\nclick==8.1.5\\nclick-log==0.4.0\\ncolorama==0.4.6\\nconfection==0.1.0\\ncymem==2.0.7\\ndataclasses-json==0.5.9\\ndeprecation==2.1.0\\ndnspython==2.3.0\\ndocutils==0.20.1\\ndotty-dict==1.3.1\\nemail-validator==2.0.0.post2\\nexceptiongroup==1.1.2\\nfastapi==0.99.1\\nfrozenlist==1.4.0\\ngitdb==4.0.10\\nGitPython==3.1.32\\ngotrue==1.0.2\\nh11==0.14.0\\nhttpcore==0.16.3\\nhttptools==0.6.0\\nhttpx==0.23.3\\nidna==3.4\\nimportlib-metadata==6.8.0\\ninvoke==1.7.3\\nitsdangerous==2.1.2\\njaraco.classes==3.3.0\\nJinja2==3.1.2\\nkeyring==24.2.0\\nlangchain==0.0.234\\nlangcodes==3.3.0\\nlangsmith==0.0.5\\nMarkupSafe==2.1.3\\nmarshmallow==3.19.0\\nmarshmallow-enum==1.5.1\\nmore-itertools==9.1.0\\nmultidict==6.0.4\\nmurmurhash==1.0.9\\nmypy-extensions==1.0.0\\nnumexpr==2.8.4\\nnumpy==1.25.1\\nopenai==0.27.8\\nopenapi-schema-pydantic==1.2.4\\norjson==3.9.2\\npackaging==23.1\\npathy==0.10.2\\npkginfo==1.9.6\\npostgrest==0.10.6\\npreshed==3.0.8\\npydantic==1.10.11\\npydantic_core==2.3.0\\nPygments==2.15.1\\npython-dateutil==2.8.2\\npython-dotenv==1.0.0\\npython-gitlab==3.15.0\\npython-multipart==0.0.6\\npython-semantic-release==7.33.2\\nPyYAML==6.0\\nreadme-renderer==40.0\\nrealtime==1.0.0\\nrequests==2.31.0\\nrequests-toolbelt==1.0.0\\nrfc3986==1.5.0\\nsemver==2.13.0\\nsix==1.16.0\\nsmart-open==6.3.0\\nsmmap==5.0.0\\nsniffio==1.3.0\\nspacy==3.6.0\\nspacy-legacy==3.0.12\\nspacy-loggers==1.0.4\\nSQLAlchemy==2.0.19\\nsrsly==2.4.6\\nstarlette==0.27.0\\nstorage3==0.5.2\\nStrEnum==0.4.15\\nsupabase==1.0.3\\nsupafunc==0.2.2\\ntenacity==8.2.2\\nthinc==8.1.10\\nthinc-apple-ops==0.1.3\\ntomlkit==0.11.8\\ntqdm==4.65.0\\ntwine==3.8.0\\ntyper==0.9.0\\ntyping-inspect==0.9.0\\ntyping_extensions==4.7.1\\nujson==5.8.0\\nurllib3==2.0.3\\nuvicorn==0.23.0\\nuvloop==0.17.0\\nwasabi==1.1.2\\nwatchfiles==0.19.0\\nwebencodings==0.5.1\\nwebsockets==10.4\\nyarl==1.9.2\\nzipp==3.16.2\\n'), GithubFile(filename='runtime.txt', content='python-3.9.17\\n'), GithubFile(filename='testbed.ipynb', content='{\\n \"cells\": [\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 8,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"\\\\n\",\\n      \"\\\\n\",\\n      \"1. Python \\\\n\",\\n      \"2. Go \\\\n\",\\n      \"3. Rust \\\\n\",\\n      \"4. C++ \\\\n\",\\n      \"5. Ruby \\\\n\",\\n      \"6. JavaScript \\\\n\",\\n      \"7. Swift \\\\n\",\\n      \"8. Elixir \\\\n\",\\n      \"9. Haskell \\\\n\",\\n      \"10. Julia\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"from langchains.openai import create_openai_llm\\\\n\",\\n    \"\\\\n\",\\n    \"llm = create_openai_llm()\\\\n\",\\n    \"\\\\n\",\\n    \"print(llm.predict(\\\\\"Cool programming languages\\\\\"))\"\\n   ]\\n  }\\n ],\\n \"metadata\": {\\n  \"kernelspec\": {\\n   \"display_name\": \"venv\",\\n   \"language\": \"python\",\\n   \"name\": \"python3\"\\n  },\\n  \"language_info\": {\\n   \"codemirror_mode\": {\\n    \"name\": \"ipython\",\\n    \"version\": 3\\n   },\\n   \"file_extension\": \".py\",\\n   \"mimetype\": \"text/x-python\",\\n   \"name\": \"python\",\\n   \"nbconvert_exporter\": \"python\",\\n   \"pygments_lexer\": \"ipython3\",\\n   \"version\": \"3.9.6\"\\n  },\\n  \"orig_nbformat\": 4\\n },\\n \"nbformat\": 4,\\n \"nbformat_minor\": 2\\n}\\n')]\n"
     ]
    }
   ],
   "source": [
    "from integrations.github import read_github_data_from_file\n",
    "\n",
    "files = read_github_data_from_file()\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ProcessedGithubFile(originalFile=GithubFile(filename='.gitignore', content='# Config secrets\\n.env\\n\\n# Python cache files\\n__pycache__/\\n*.py[cod]\\n\\n# Virtual environment\\nvenv/\\n\\n# Project-specific settings\\n.env  # environment variables, never commit this\\n*.log  # log files\\n\\n# Byte-compiled / optimized / DLL files\\n*.py[cod]\\n*$py.class\\n\\n# Distribution / packaging\\n.Python\\nbuild/\\ndevelop-eggs/\\ndist/\\ndownloads/\\neggs/\\n.eggs/\\nlib/\\nlib64/\\nparts/\\nsdist/\\nvar/\\n*.egg-info/\\n.installed.cfg\\n*.egg\\n\\n# PyCharm files\\n.idea/\\n*.iml'), llmResponse='\\n    0'), ProcessedGithubFile(originalFile=GithubFile(filename='Procfile', content='web: uvicorn main:app --host=0.0.0.0 --port=${PORT:-5000}'), llmResponse='\\n    0'), ProcessedGithubFile(originalFile=GithubFile(filename='README.md', content='# TinyGen\\n\\nA simple API to generate code diffs using ChatGPT\\n\\n## Running the Application Locally\\n\\n1. First, clone the repository to your local machine and navigate to the project directory:\\n\\n```bash\\ngit clone <repository-url>\\ncd <project-directory>\\nsource venv/bin/activate\\npip install -r requirements.txt\\n```\\n\\nTeardown:\\n```bash\\ndeactivate\\n```\\n\\n2. create a new .env file for secrets, you can get this information from [supabase](https://supabase.com/dashboard/project/fvvmbtjoztejtalynctc/settings/api)\\nyour .env file should look like this:\\n```\\nSUPABASE_URL=your_supabase_url\\nSUPABASE_API_KEY=your_supabase_api_key\\n```\\n\\n\\n3. Run the app\\n```bash\\nuvicorn main:app --reload\\n```\\n\\nNow, the server should be up and running at http://localhost:8000. You can view the interactive API documentation at http://localhost:8000/docs.\\n\\nTo test your server, you can run the following CURL command:\\n```bash\\ncurl http://localhost:8000/test\\n```\\n\\nTo make a proper request to generate a response, and record the input and output in supabase, run the following CURL\\n```bash\\ncurl -X POST -H \"Content-Type: application/json\" -d \\'{\"url\": \"yourgitrepo.url\", \"prompt\": \"your prompt\"}\\' http://127.0.0.1:8000/generate\\n```\\n\\n\\n## Deploy changes to prod\\nUse Heroku for production:\\n```bash\\nheroku login\\ngit push heroku main\\nheroku open\\nheroku logs --tail\\n```\\n\\n\\n### Python things:\\n```bash\\nsource venv/bin/activate # activate virtual env\\ndeactive # deactivate virtual env\\npip freeze > requirements.txt # update deps\\n```'), llmResponse='\\n    0'), ProcessedGithubFile(originalFile=GithubFile(filename='config.py', content='from dotenv import load_dotenv\\nimport os\\n\\nload_dotenv() # load local .env file if it exists for local development\\n\\nsupabase_url = os.getenv(\"SUPABASE_URL\")\\nsupabase_api_key = os.getenv(\"SUPABASE_API_KEY\")\\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\")'), llmResponse='\\n    0'), ProcessedGithubFile(originalFile=GithubFile(filename='main.py', content='from fastapi import FastAPI\\nfrom app.models import RequestRecord\\nfrom db.supabase import supabase_record_request\\nfrom langchains.openai import create_openai_llm\\nfrom integrations.github import get_github_files_and_contents\\n\\napp = FastAPI()\\n\\nllm = create_openai_llm()\\n\\n@app.get(\"/test\")\\ndef read_test():\\n    return {\"message\": \"hello\"}\\n\\n@app.get(\"/testllm\")\\ndef test_llm():\\n    return llm.predict(\"What is a cool name for a dog?\")\\n\\n@app.get(\"/testgithub\")\\ndef test_github():\\n    return get_github_files_and_contents(\\'https://github.com/chasemc67/TinyGen\\')\\n\\n# Append to the history table log\\n@app.post(\"/generate\")\\ndef generate(request: RequestRecord):\\n    try:\\n        # Add request into history table\\n        request = supabase_record_request(request)\\n\\n        if request:\\n            print(\"Request recorded successfully\")\\n        else: \\n            return {\"message\": \"Request recording failed\"}\\n    except Exception as e:\\n        print(\"Error: \", e)\\n        return {\"message\": \"Request recording failed\"}\\n\\n    # TODO get files from github to pass to LLM (or invoke LLM now with custom github tool)\\n\\n    return {\"message\": \"Request recording succeeded\"}'), llmResponse='\\n    50'), ProcessedGithubFile(originalFile=GithubFile(filename='requirements.txt', content='aiohttp==3.8.4\\naiosignal==1.3.1\\nannotated-types==0.5.0\\nanyio==3.7.1\\nasync-timeout==4.0.2\\nattrs==23.1.0\\nbcrypt==4.0.1\\nbleach==6.0.0\\nblis==0.7.9\\ncatalogue==2.0.8\\ncertifi==2023.5.7\\ncharset-normalizer==3.2.0\\nclick==8.1.5\\nclick-log==0.4.0\\ncolorama==0.4.6\\nconfection==0.1.0\\ncymem==2.0.7\\ndataclasses-json==0.5.9\\ndeprecation==2.1.0\\ndnspython==2.3.0\\ndocutils==0.20.1\\ndotty-dict==1.3.1\\nemail-validator==2.0.0.post2\\nexceptiongroup==1.1.2\\nfastapi==0.99.1\\nfrozenlist==1.4.0\\ngitdb==4.0.10\\nGitPython==3.1.32\\ngotrue==1.0.2\\nh11==0.14.0\\nhttpcore==0.16.3\\nhttptools==0.6.0\\nhttpx==0.23.3\\nidna==3.4\\nimportlib-metadata==6.8.0\\ninvoke==1.7.3\\nitsdangerous==2.1.2\\njaraco.classes==3.3.0\\nJinja2==3.1.2\\nkeyring==24.2.0\\nlangchain==0.0.234\\nlangcodes==3.3.0\\nlangsmith==0.0.5\\nMarkupSafe==2.1.3\\nmarshmallow==3.19.0\\nmarshmallow-enum==1.5.1\\nmore-itertools==9.1.0\\nmultidict==6.0.4\\nmurmurhash==1.0.9\\nmypy-extensions==1.0.0\\nnumexpr==2.8.4\\nnumpy==1.25.1\\nopenai==0.27.8\\nopenapi-schema-pydantic==1.2.4\\norjson==3.9.2\\npackaging==23.1\\npathy==0.10.2\\npkginfo==1.9.6\\npostgrest==0.10.6\\npreshed==3.0.8\\npydantic==1.10.11\\npydantic_core==2.3.0\\nPygments==2.15.1\\npython-dateutil==2.8.2\\npython-dotenv==1.0.0\\npython-gitlab==3.15.0\\npython-multipart==0.0.6\\npython-semantic-release==7.33.2\\nPyYAML==6.0\\nreadme-renderer==40.0\\nrealtime==1.0.0\\nrequests==2.31.0\\nrequests-toolbelt==1.0.0\\nrfc3986==1.5.0\\nsemver==2.13.0\\nsix==1.16.0\\nsmart-open==6.3.0\\nsmmap==5.0.0\\nsniffio==1.3.0\\nspacy==3.6.0\\nspacy-legacy==3.0.12\\nspacy-loggers==1.0.4\\nSQLAlchemy==2.0.19\\nsrsly==2.4.6\\nstarlette==0.27.0\\nstorage3==0.5.2\\nStrEnum==0.4.15\\nsupabase==1.0.3\\nsupafunc==0.2.2\\ntenacity==8.2.2\\nthinc==8.1.10\\nthinc-apple-ops==0.1.3\\ntomlkit==0.11.8\\ntqdm==4.65.0\\ntwine==3.8.0\\ntyper==0.9.0\\ntyping-inspect==0.9.0\\ntyping_extensions==4.7.1\\nujson==5.8.0\\nurllib3==2.0.3\\nuvicorn==0.23.0\\nuvloop==0.17.0\\nwasabi==1.1.2\\nwatchfiles==0.19.0\\nwebencodings==0.5.1\\nwebsockets==10.4\\nyarl==1.9.2\\nzipp==3.16.2\\n'), llmResponse='\\n    0'), ProcessedGithubFile(originalFile=GithubFile(filename='runtime.txt', content='python-3.9.17\\n'), llmResponse=' 0'), ProcessedGithubFile(originalFile=GithubFile(filename='testbed.ipynb', content='{\\n \"cells\": [\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 8,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"\\\\n\",\\n      \"\\\\n\",\\n      \"1. Python \\\\n\",\\n      \"2. Go \\\\n\",\\n      \"3. Rust \\\\n\",\\n      \"4. C++ \\\\n\",\\n      \"5. Ruby \\\\n\",\\n      \"6. JavaScript \\\\n\",\\n      \"7. Swift \\\\n\",\\n      \"8. Elixir \\\\n\",\\n      \"9. Haskell \\\\n\",\\n      \"10. Julia\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"from langchains.openai import create_openai_llm\\\\n\",\\n    \"\\\\n\",\\n    \"llm = create_openai_llm()\\\\n\",\\n    \"\\\\n\",\\n    \"print(llm.predict(\\\\\"Cool programming languages\\\\\"))\"\\n   ]\\n  }\\n ],\\n \"metadata\": {\\n  \"kernelspec\": {\\n   \"display_name\": \"venv\",\\n   \"language\": \"python\",\\n   \"name\": \"python3\"\\n  },\\n  \"language_info\": {\\n   \"codemirror_mode\": {\\n    \"name\": \"ipython\",\\n    \"version\": 3\\n   },\\n   \"file_extension\": \".py\",\\n   \"mimetype\": \"text/x-python\",\\n   \"name\": \"python\",\\n   \"nbconvert_exporter\": \"python\",\\n   \"pygments_lexer\": \"ipython3\",\\n   \"version\": \"3.9.6\"\\n  },\\n  \"orig_nbformat\": 4\\n },\\n \"nbformat\": 4,\\n \"nbformat_minor\": 2\\n}\\n'), llmResponse='\\n    0')]\n",
      "[ProcessedGithubFile(originalFile=GithubFile(filename='.gitignore', content='# Config secrets\\n.env\\n\\n# Python cache files\\n__pycache__/\\n*.py[cod]\\n\\n# Virtual environment\\nvenv/\\n\\n# Project-specific settings\\n.env  # environment variables, never commit this\\n*.log  # log files\\n\\n# Byte-compiled / optimized / DLL files\\n*.py[cod]\\n*$py.class\\n\\n# Distribution / packaging\\n.Python\\nbuild/\\ndevelop-eggs/\\ndist/\\ndownloads/\\neggs/\\n.eggs/\\nlib/\\nlib64/\\nparts/\\nsdist/\\nvar/\\n*.egg-info/\\n.installed.cfg\\n*.egg\\n\\n# PyCharm files\\n.idea/\\n*.iml'), llmResponse='\\n    0'), ProcessedGithubFile(originalFile=GithubFile(filename='Procfile', content='web: uvicorn main:app --host=0.0.0.0 --port=${PORT:-5000}'), llmResponse='\\n    0'), ProcessedGithubFile(originalFile=GithubFile(filename='README.md', content='# TinyGen\\n\\nA simple API to generate code diffs using ChatGPT\\n\\n## Running the Application Locally\\n\\n1. First, clone the repository to your local machine and navigate to the project directory:\\n\\n```bash\\ngit clone <repository-url>\\ncd <project-directory>\\nsource venv/bin/activate\\npip install -r requirements.txt\\n```\\n\\nTeardown:\\n```bash\\ndeactivate\\n```\\n\\n2. create a new .env file for secrets, you can get this information from [supabase](https://supabase.com/dashboard/project/fvvmbtjoztejtalynctc/settings/api)\\nyour .env file should look like this:\\n```\\nSUPABASE_URL=your_supabase_url\\nSUPABASE_API_KEY=your_supabase_api_key\\n```\\n\\n\\n3. Run the app\\n```bash\\nuvicorn main:app --reload\\n```\\n\\nNow, the server should be up and running at http://localhost:8000. You can view the interactive API documentation at http://localhost:8000/docs.\\n\\nTo test your server, you can run the following CURL command:\\n```bash\\ncurl http://localhost:8000/test\\n```\\n\\nTo make a proper request to generate a response, and record the input and output in supabase, run the following CURL\\n```bash\\ncurl -X POST -H \"Content-Type: application/json\" -d \\'{\"url\": \"yourgitrepo.url\", \"prompt\": \"your prompt\"}\\' http://127.0.0.1:8000/generate\\n```\\n\\n\\n## Deploy changes to prod\\nUse Heroku for production:\\n```bash\\nheroku login\\ngit push heroku main\\nheroku open\\nheroku logs --tail\\n```\\n\\n\\n### Python things:\\n```bash\\nsource venv/bin/activate # activate virtual env\\ndeactive # deactivate virtual env\\npip freeze > requirements.txt # update deps\\n```'), llmResponse='\\n    0'), ProcessedGithubFile(originalFile=GithubFile(filename='config.py', content='from dotenv import load_dotenv\\nimport os\\n\\nload_dotenv() # load local .env file if it exists for local development\\n\\nsupabase_url = os.getenv(\"SUPABASE_URL\")\\nsupabase_api_key = os.getenv(\"SUPABASE_API_KEY\")\\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\")'), llmResponse='\\n    0'), ProcessedGithubFile(originalFile=GithubFile(filename='main.py', content='from fastapi import FastAPI\\nfrom app.models import RequestRecord\\nfrom db.supabase import supabase_record_request\\nfrom langchains.openai import create_openai_llm\\nfrom integrations.github import get_github_files_and_contents\\n\\napp = FastAPI()\\n\\nllm = create_openai_llm()\\n\\n@app.get(\"/test\")\\ndef read_test():\\n    return {\"message\": \"hello\"}\\n\\n@app.get(\"/testllm\")\\ndef test_llm():\\n    return llm.predict(\"What is a cool name for a dog?\")\\n\\n@app.get(\"/testgithub\")\\ndef test_github():\\n    return get_github_files_and_contents(\\'https://github.com/chasemc67/TinyGen\\')\\n\\n# Append to the history table log\\n@app.post(\"/generate\")\\ndef generate(request: RequestRecord):\\n    try:\\n        # Add request into history table\\n        request = supabase_record_request(request)\\n\\n        if request:\\n            print(\"Request recorded successfully\")\\n        else: \\n            return {\"message\": \"Request recording failed\"}\\n    except Exception as e:\\n        print(\"Error: \", e)\\n        return {\"message\": \"Request recording failed\"}\\n\\n    # TODO get files from github to pass to LLM (or invoke LLM now with custom github tool)\\n\\n    return {\"message\": \"Request recording succeeded\"}'), llmResponse='\\n    50'), ProcessedGithubFile(originalFile=GithubFile(filename='requirements.txt', content='aiohttp==3.8.4\\naiosignal==1.3.1\\nannotated-types==0.5.0\\nanyio==3.7.1\\nasync-timeout==4.0.2\\nattrs==23.1.0\\nbcrypt==4.0.1\\nbleach==6.0.0\\nblis==0.7.9\\ncatalogue==2.0.8\\ncertifi==2023.5.7\\ncharset-normalizer==3.2.0\\nclick==8.1.5\\nclick-log==0.4.0\\ncolorama==0.4.6\\nconfection==0.1.0\\ncymem==2.0.7\\ndataclasses-json==0.5.9\\ndeprecation==2.1.0\\ndnspython==2.3.0\\ndocutils==0.20.1\\ndotty-dict==1.3.1\\nemail-validator==2.0.0.post2\\nexceptiongroup==1.1.2\\nfastapi==0.99.1\\nfrozenlist==1.4.0\\ngitdb==4.0.10\\nGitPython==3.1.32\\ngotrue==1.0.2\\nh11==0.14.0\\nhttpcore==0.16.3\\nhttptools==0.6.0\\nhttpx==0.23.3\\nidna==3.4\\nimportlib-metadata==6.8.0\\ninvoke==1.7.3\\nitsdangerous==2.1.2\\njaraco.classes==3.3.0\\nJinja2==3.1.2\\nkeyring==24.2.0\\nlangchain==0.0.234\\nlangcodes==3.3.0\\nlangsmith==0.0.5\\nMarkupSafe==2.1.3\\nmarshmallow==3.19.0\\nmarshmallow-enum==1.5.1\\nmore-itertools==9.1.0\\nmultidict==6.0.4\\nmurmurhash==1.0.9\\nmypy-extensions==1.0.0\\nnumexpr==2.8.4\\nnumpy==1.25.1\\nopenai==0.27.8\\nopenapi-schema-pydantic==1.2.4\\norjson==3.9.2\\npackaging==23.1\\npathy==0.10.2\\npkginfo==1.9.6\\npostgrest==0.10.6\\npreshed==3.0.8\\npydantic==1.10.11\\npydantic_core==2.3.0\\nPygments==2.15.1\\npython-dateutil==2.8.2\\npython-dotenv==1.0.0\\npython-gitlab==3.15.0\\npython-multipart==0.0.6\\npython-semantic-release==7.33.2\\nPyYAML==6.0\\nreadme-renderer==40.0\\nrealtime==1.0.0\\nrequests==2.31.0\\nrequests-toolbelt==1.0.0\\nrfc3986==1.5.0\\nsemver==2.13.0\\nsix==1.16.0\\nsmart-open==6.3.0\\nsmmap==5.0.0\\nsniffio==1.3.0\\nspacy==3.6.0\\nspacy-legacy==3.0.12\\nspacy-loggers==1.0.4\\nSQLAlchemy==2.0.19\\nsrsly==2.4.6\\nstarlette==0.27.0\\nstorage3==0.5.2\\nStrEnum==0.4.15\\nsupabase==1.0.3\\nsupafunc==0.2.2\\ntenacity==8.2.2\\nthinc==8.1.10\\nthinc-apple-ops==0.1.3\\ntomlkit==0.11.8\\ntqdm==4.65.0\\ntwine==3.8.0\\ntyper==0.9.0\\ntyping-inspect==0.9.0\\ntyping_extensions==4.7.1\\nujson==5.8.0\\nurllib3==2.0.3\\nuvicorn==0.23.0\\nuvloop==0.17.0\\nwasabi==1.1.2\\nwatchfiles==0.19.0\\nwebencodings==0.5.1\\nwebsockets==10.4\\nyarl==1.9.2\\nzipp==3.16.2\\n'), llmResponse='\\n    0'), ProcessedGithubFile(originalFile=GithubFile(filename='runtime.txt', content='python-3.9.17\\n'), llmResponse=' 0'), ProcessedGithubFile(originalFile=GithubFile(filename='testbed.ipynb', content='{\\n \"cells\": [\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 8,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"\\\\n\",\\n      \"\\\\n\",\\n      \"1. Python \\\\n\",\\n      \"2. Go \\\\n\",\\n      \"3. Rust \\\\n\",\\n      \"4. C++ \\\\n\",\\n      \"5. Ruby \\\\n\",\\n      \"6. JavaScript \\\\n\",\\n      \"7. Swift \\\\n\",\\n      \"8. Elixir \\\\n\",\\n      \"9. Haskell \\\\n\",\\n      \"10. Julia\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"from langchains.openai import create_openai_llm\\\\n\",\\n    \"\\\\n\",\\n    \"llm = create_openai_llm()\\\\n\",\\n    \"\\\\n\",\\n    \"print(llm.predict(\\\\\"Cool programming languages\\\\\"))\"\\n   ]\\n  }\\n ],\\n \"metadata\": {\\n  \"kernelspec\": {\\n   \"display_name\": \"venv\",\\n   \"language\": \"python\",\\n   \"name\": \"python3\"\\n  },\\n  \"language_info\": {\\n   \"codemirror_mode\": {\\n    \"name\": \"ipython\",\\n    \"version\": 3\\n   },\\n   \"file_extension\": \".py\",\\n   \"mimetype\": \"text/x-python\",\\n   \"name\": \"python\",\\n   \"nbconvert_exporter\": \"python\",\\n   \"pygments_lexer\": \"ipython3\",\\n   \"version\": \"3.9.6\"\\n  },\\n  \"orig_nbformat\": 4\\n },\\n \"nbformat\": 4,\\n \"nbformat_minor\": 2\\n}\\n'), llmResponse='\\n    0')]\n"
     ]
    }
   ],
   "source": [
    "from app.generate_diff import find_relevant_files_for_prompt_and_repo\n",
    "\n",
    "processedFiles = await find_relevant_files_for_prompt_and_repo(\"The file where the routes are defined. Should contain things like app.get\", \"https://github.com/chasemc67/TinyGen\")\n",
    "print(processedFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".gitignore \n",
      "    0\n",
      "\n",
      "\n",
      "Procfile \n",
      "    0\n",
      "\n",
      "\n",
      "README.md \n",
      "    0\n",
      "\n",
      "\n",
      "config.py \n",
      "    0\n",
      "\n",
      "\n",
      "main.py \n",
      "    50\n",
      "\n",
      "\n",
      "requirements.txt \n",
      "    0\n",
      "\n",
      "\n",
      "runtime.txt  0\n",
      "\n",
      "\n",
      "testbed.ipynb \n",
      "    0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in processedFiles:\n",
    "    print(file.originalFile.filename, file.llmResponse)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Answer: You have given the userPrompt as \"what the user wants\" and the codePrompt as \"the users code\".\n"
     ]
    }
   ],
   "source": [
    "from langchains.openai import run_test_llm_chain\n",
    "\n",
    "result = run_test_llm_chain()\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
